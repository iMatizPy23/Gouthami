{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1FTEhR60id"
      },
      "source": [
        "# Sentiment Analysis with Deep Learning using Mocdified BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnA4pJIr60ig"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHt3fKIg60ih"
      },
      "source": [
        "#libraries and packages\n",
        "!pip install torch\n",
        "!pip install ranger-adabelief==0.1.0 \n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SQJra4t9z5Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "import time\n",
        "import jsonpickle\n"
      ],
      "metadata": {
        "id": "nQ8vs9bD_KlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consume:\n",
        "CONSUMER_KEY = 'SS97NM79TEdKPsSWlmnhUcOwd'\n",
        "CONSUMER_SECRET = 'rZpFgeEMsCp6cYZ4u2gSTVj9sFXQUMVh5lVkDAEV2b3MTEGyE2'\n",
        "\n",
        "# Access:\n",
        "ACCESS_TOKEN = '1037289423321350144-aG0QZqLs0cOHIuwNgagYIRTZVT40b4'\n",
        "ACCESS_SECRET = 'O4qSyIY74M7iUqQB2Qgb92H8AwIvCV5O3SjSA8rJFbc10'\n",
        "\n",
        "\n",
        "def connect_to_twitter_OAuth():\n",
        "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
        "\n",
        "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "    return api\n",
        "\n",
        "\n",
        "# Create API object\n",
        "api = connect_to_twitter_OAuth()"
      ],
      "metadata": {
        "id": "U51k6Iuc_KfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Extracting the tweets\"\"\"\n",
        "\n",
        "def get_save_tweets(filepath, api, query,max_tweets=1000, lang='en'):\n",
        "    tweetCount = 0\n",
        "\n",
        "    # Open file and save tweets\n",
        "    with open(filepath, 'w') as f:\n",
        "        # Send the query\n",
        "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang=lang, since_id='2017-01-01').items(max_tweets):\n",
        "        # for tweet in tweepy.Cursor(api.search_tweets, q=query, lang=lang).items(max_tweets):\n",
        "            # Convert to JSON format\n",
        "            f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
        "            tweetCount += 1\n",
        "\n",
        "        # Display how many tweets we have collected\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "query = 'airlines'# change the query topic Ex:'BuzzFeed' for Business tweets \n",
        "get_save_tweets('Airlines.json', api, query)# change the filename \".json\" for different dataset"
      ],
      "metadata": {
        "id": "7kUnbq1l_RgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANkXIuV8DNmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweets_to_df(path):\n",
        "    tweets = list(open(path, 'rt'))\n",
        "\n",
        "    text = []\n",
        "    weekday = []\n",
        "    month = []\n",
        "    day = []\n",
        "    hour = []\n",
        "    hashtag = []\n",
        "    url = []\n",
        "    favorite = []\n",
        "    reply = []\n",
        "    retweet = []\n",
        "    follower = []\n",
        "    following = []\n",
        "    user = []\n",
        "    screen_name = []\n",
        "\n",
        "    for t in tweets:\n",
        "        t = jsonpickle.decode(t)\n",
        "\n",
        "        # Text\n",
        "        text.append(t['text'])\n",
        "\n",
        "        # Decompose date\n",
        "        date = t['created_at']\n",
        "        weekday.append(date.split(' ')[0])\n",
        "        month.append(date.split(' ')[1])\n",
        "        day.append(date.split(' ')[2])\n",
        "\n",
        "        time = date.split(' ')[3].split(':')\n",
        "        hour.append(time[0])\n",
        "\n",
        "        # Has hashtag\n",
        "        if len(t['entities']['hashtags']) == 0:\n",
        "            hashtag.append(0)\n",
        "        else:\n",
        "            hashtag.append(1)\n",
        "\n",
        "        # Has url\n",
        "        if len(t['entities']['urls']) == 0:\n",
        "            url.append(0)\n",
        "        else:\n",
        "            url.append(1)\n",
        "\n",
        "        # Number of favs\n",
        "        favorite.append(t['favorite_count'])\n",
        "\n",
        "        # Is reply?\n",
        "        if t['in_reply_to_status_id'] == None:\n",
        "            reply.append(0)\n",
        "        else:\n",
        "            reply.append(1)\n",
        "\n",
        "            # Retweets count\n",
        "        retweet.append(t['retweet_count'])\n",
        "\n",
        "        # Followers number\n",
        "        follower.append(t['user']['followers_count'])\n",
        "\n",
        "        # Following number\n",
        "        following.append(t['user']['friends_count'])\n",
        "\n",
        "        # Add user\n",
        "        user.append(t['user']['name'])\n",
        "\n",
        "        # Add screen name\n",
        "        screen_name.append(t['user']['screen_name'])\n",
        "\n",
        "    d = {'TweetText': text,\n",
        "         'weekday': weekday,\n",
        "         'month': month,\n",
        "         'day': day,\n",
        "         'hour': hour,\n",
        "         'has_hashtag': hashtag,\n",
        "         'has_url': url,\n",
        "         'fav_count': favorite,\n",
        "         'is_reply': reply,\n",
        "         'retweet_count': retweet,\n",
        "         'followers': follower,\n",
        "         'following': following,\n",
        "         'user': user,\n",
        "         'screen_name': screen_name\n",
        "         }\n",
        "\n",
        "    return pd.DataFrame(data=d)\n",
        "df =  tweets_to_df('Airlines.json')"
      ],
      "metadata": {
        "id": "jCEtAldb_1W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"AirlinesTweets.csv\",sep = \",\", index=False)"
      ],
      "metadata": {
        "id": "qcaGxB81_1Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D11PuRbf60ii"
      },
      "source": [
        "#load data\n",
        "df = pd.read_csv('AirlinesTweets.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-qd9eg60ii"
      },
      "source": [
        "#preview\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHdnqzOrXhbc"
      },
      "source": [
        "#info\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKV9m1cVXuKa"
      },
      "source": [
        "#check for null\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX5ry-Cv60ii"
      },
      "source": [
        "#look at an example\n",
        "df.TweetText.iloc[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg3-5WLA60ij"
      },
      "source": [
        "#count for each class\n",
        "df.day.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fbpB9_sXzeV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# #plot class distribution\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.countplot(df.lsentiment, palette='Spectral')\n",
        "# plt.xlabel('Classes')\n",
        "# plt.title('Class Distribution');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHwGMIQ060ik"
      },
      "source": [
        "#store classes into an array\n",
        "possible_labels = df.day.unique()\n",
        "possible_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert labels into numeric values\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index"
      ],
      "metadata": {
        "id": "bXpw7ajNlFHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert labels into numeric values\n",
        "df['day'] = df.day.replace(label_dict)\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "TV2a5hDxnYtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFG7Chi2uvV_"
      },
      "source": [
        "#need equal length sentences\n",
        "#plot hist of sentence length\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot([len(s) for s in df.TweetText], bins=100)\n",
        "plt.title('Sentence Length')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-WPcgPt_C-a"
      },
      "source": [
        "#find the maximum length\n",
        "max_len = max([len(sent) for sent in df.TweetText])\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OxAO8V60im"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rMo_yLa60im"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
        "                                                   df.day.values,\n",
        "                                                   test_size = 0.2,\n",
        "                                                   random_state = 17,\n",
        "                                                   stratify = df.day.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAYlqafy60im"
      },
      "source": [
        "#create new column\n",
        "df['data_type'] = ['not_set'] * df.shape[0]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE19B_ZE60im"
      },
      "source": [
        "#fill in data type\n",
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j69pOECW60in"
      },
      "source": [
        "df.groupby(['day', 'data_type']).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtStI_gV60in"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xZOb7_460in"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgi9Hex160in"
      },
      "source": [
        "#load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                         do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhxR0qU60io"
      },
      "source": [
        "#tokenize train set\n",
        "encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type == 'train'].TweetText.values,\n",
        "                                                add_special_tokens = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 150,\n",
        "                                                return_tensors = 'pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwST8V-460io"
      },
      "source": [
        "#tokenizer val set\n",
        "encoded_data_val = tokenizer.batch_encode_plus(df[df.data_type == 'val'].TweetText.values,\n",
        "                                                #add_special_tokens = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 150,\n",
        "                                                return_tensors = 'pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNqxw0OqaUQO"
      },
      "source": [
        "encoded_data_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi6YpJ3k8nIF"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHetAk2i60ip"
      },
      "source": [
        "#encode train set\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(df[df.data_type == 'train'].day.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ajiwDtl60ip"
      },
      "source": [
        "#encode val set\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "\n",
        "#convert data type to torch.tensor\n",
        "labels_val = torch.tensor(df[df.data_type == 'val'].day.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDtS0GaEAi"
      },
      "source": [
        "input_ids_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfsi2jqEaGRS"
      },
      "source": [
        "attention_masks_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwy2ZLYCZ93j"
      },
      "source": [
        "labels_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKZIof2460ip"
      },
      "source": [
        "#create dataloader\n",
        "dataset_train = TensorDataset(input_ids_train, \n",
        "                              attention_masks_train,\n",
        "                              labels_train)\n",
        "\n",
        "dataset_val = TensorDataset(input_ids_val, \n",
        "                             attention_masks_val, \n",
        "                             labels_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF5enP4w60ip"
      },
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KffhsXK0Zzb-"
      },
      "source": [
        "dataset_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Q9oLTiZ3-L"
      },
      "source": [
        "dataset_train.tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latJzGF_60iq"
      },
      "source": [
        "## Set Up BERT Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUYs26we60iq"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "#load pre-trained BERT\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels = len(label_dict),\n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q62qAIzNnOE"
      },
      "source": [
        "#model summary\n",
        "model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D3puesh60iq"
      },
      "source": [
        "## Create Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FgDKKic60ir"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 4 #since we have limited resource\n",
        "\n",
        "#load train set\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler = RandomSampler(dataset_train),\n",
        "                              batch_size = batch_size)\n",
        "\n",
        "#load val set\n",
        "dataloader_val = DataLoader(dataset_val,\n",
        "                              sampler = RandomSampler(dataset_val),\n",
        "                              batch_size = 32) #since we don't have to do backpropagation for this step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1Z2UTK60ir"
      },
      "source": [
        "from transformers import Adafactor,get_linear_schedule_with_warmup\n",
        "epochs = 10\n",
        "#load optimizer\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer = RangerAdaBelief(model.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhH94Xsc60is"
      },
      "source": [
        "#load scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = len(dataloader_train)*epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HzioXyS60is"
      },
      "source": [
        "## Define Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZVIyusm60it"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# #f1 score\n",
        "# def f1_score_func(preds, labels):\n",
        "#     preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "#     labels_flat = labels.flatten()\n",
        "#     return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
        "def eval_metrics(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = metrics.accuracy_score(labels_flat, preds_flat)\n",
        "    prec = metrics.precision_score(labels_flat, preds_flat, average = 'weighted')\n",
        "    recall = metrics.recall_score(labels_flat, preds_flat, average = 'weighted')\n",
        "    f1score = metrics.f1_score(labels_flat, preds_flat, average = 'weighted')\n",
        "    conf_mat= metrics.confusion_matrix(labels_flat, preds_flat)\n",
        "    return acc,prec,recall,f1score,conf_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVW4nhU460it"
      },
      "source": [
        "#accuracy score\n",
        "def accuracy_per_class(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "    \n",
        "    #make prediction\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    \n",
        "    for label in np.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat==label]\n",
        "        y_true = labels_flat[labels_flat==label]\n",
        "        print(f'Class: {label_dict_inverse[label]}')\n",
        "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtoB8WJ760iu"
      },
      "source": [
        "def evaluate(dataloader_val):\n",
        "\n",
        "    #evaluation mode disables the dropout layer \n",
        "    model.eval()\n",
        "    \n",
        "    #tracking variables\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in tqdm(dataloader_val):\n",
        "        \n",
        "        #load into GPU\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        #define inputs\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2]}\n",
        "\n",
        "        #compute logits\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        #compute loss\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        #compute accuracy\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    #compute average loss\n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_val_avg, predictions, true_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj-sNTht60it"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8o352rJ60iu"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_NqfMhBK6ap"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61xRhnOR60iu"
      },
      "source": [
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "# for epoch in range(epoch_num):\n",
        "\n",
        "    #set model in train mode\n",
        "    model.train()\n",
        "\n",
        "    #tracking variable\n",
        "    loss_train_total = 0\n",
        "    \n",
        "    #set up progress bar\n",
        "    progress_bar = tqdm(dataloader_train, \n",
        "                        desc='Epoch {:1d}'.format(epoch), \n",
        "                        leave=False, \n",
        "                        disable=False)\n",
        "    \n",
        "    for batch in progress_bar:\n",
        "        #set gradient to 0\n",
        "        model.zero_grad()\n",
        "\n",
        "        #load into GPU\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        #define inputs\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels': batch[2]}\n",
        "        \n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0] #output.loss\n",
        "        loss_train_total +=loss.item()\n",
        "\n",
        "        #backward pass to get gradients\n",
        "        # loss.backward()\n",
        "        \n",
        "        #clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        #update optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        #update scheduler\n",
        "        scheduler.step()\n",
        "        \n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n",
        "    \n",
        "    tqdm.write('\\nEpoch {epoch}')\n",
        "    \n",
        "    #print training result\n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "    \n",
        "    #evaluate\n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
        "    #f1 score\n",
        "    Acc,Precision,Recall,F1_Score,conf_mat = eval_metrics(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'Accuracy: {Acc}')\n",
        "    tqdm.write(f'Precision (weighted): {Precision}')\n",
        "    tqdm.write(f'Recall (weighted): {Recall}')\n",
        "    tqdm.write(f'F1 Score (weighted): {F1_Score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qFnCdcQ60iv"
      },
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCXK0jBroacP"
      },
      "source": [
        "outputs.loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6vvCcbYozyG"
      },
      "source": [
        "outputs.logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlL3SQkT60iv"
      },
      "source": [
        "#save model\n",
        "model.to(device)\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmh5VSUk60iw"
      },
      "source": [
        "#evaluate\n",
        "_, predictions, true_vals = evaluate(dataloader_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvwQWQPa60iw"
      },
      "source": [
        "#get accuracy score\n",
        "accuracy_per_class(predictions, true_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Acc,Precision,Recall,F1_Score,conf_matrix = eval_metrics(predictions, true_vals)"
      ],
      "metadata": {
        "id": "IIG8xLBld4l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",Acc, \"Precision:\",Precision,\"Recall :\",Recall,\"F1_Score:\",F1_Score)"
      ],
      "metadata": {
        "id": "o7qyyQ2BeTFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics._plot.confusion_matrix import confusion_matrix\n",
        "print(\"confusion_matrix\",conf_matrix)"
      ],
      "metadata": {
        "id": "DaAtiiqQEPN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "id": "ozSyJ_4YTQLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}