{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htDyMQLY25dX"
      },
      "outputs": [],
      "source": [
        "# #import install\n",
        "!pip install tweepy\n",
        "!pip install seaborn\n",
        "!pip install textblob\n",
        "!pip install nltk\n",
        "!pip install jsonpickle\n",
        "#libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uncjmiw25da"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tweepy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "import time\n",
        "import jsonpickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "iyDa46xrBDnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Consume:\n",
        "CONSUMER_KEY = 'SS97NM79TEdKPsSWlmnhUcOwd'\n",
        "CONSUMER_SECRET = 'rZpFgeEMsCp6cYZ4u2gSTVj9sFXQUMVh5lVkDAEV2b3MTEGyE2'\n",
        "\n",
        "# Access:\n",
        "ACCESS_TOKEN = '1037289423321350144-aG0QZqLs0cOHIuwNgagYIRTZVT40b4'\n",
        "ACCESS_SECRET = 'O4qSyIY74M7iUqQB2Qgb92H8AwIvCV5O3SjSA8rJFbc10'\n",
        "\n",
        "\n",
        "def connect_to_twitter_OAuth():\n",
        "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
        "\n",
        "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "    return api\n",
        "\n",
        "\n",
        "# Create API object\n",
        "api = connect_to_twitter_OAuth()"
      ],
      "metadata": {
        "id": "JA3fl7837d80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the tweets"
      ],
      "metadata": {
        "id": "p7Bfvt7rBTAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Extracting the tweets\"\"\"\n",
        "\n",
        "def get_save_tweets(filepath, api, query,max_tweets=1000, lang='en'):\n",
        "    tweetCount = 0\n",
        "\n",
        "    # Open file and save tweets\n",
        "    with open(filepath, 'w') as f:\n",
        "        # Send the query\n",
        "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang=lang, since_id='2017-01-01').items(max_tweets):\n",
        "        # for tweet in tweepy.Cursor(api.search_tweets, q=query, lang=lang).items(max_tweets):\n",
        "            # Convert to JSON format\n",
        "            f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
        "            tweetCount += 1\n",
        "\n",
        "        # Display how many tweets we have collected\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "query = 'airlines'# change the query topic Ex:'BuzzFeed' for Business tweets \n",
        "get_save_tweets('Airlines.json', api, query)# change the filename \".json\" for different dataset\n"
      ],
      "metadata": {
        "id": "8166FnHk7k9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the dataframe for extracted tweets"
      ],
      "metadata": {
        "id": "2ZInqsY4BYNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRgad44M25db"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Create the dataframe for extracted tweets\"\"\"\n",
        "\n",
        "def tweets_to_df(path):\n",
        "    tweets = list(open(path, 'rt'))\n",
        "\n",
        "    text = []\n",
        "    weekday = []\n",
        "    month = []\n",
        "    day = []\n",
        "    hour = []\n",
        "    user = []\n",
        "    \n",
        "\n",
        "    for t in tweets:\n",
        "        t = jsonpickle.decode(t)\n",
        "\n",
        "        # Text\n",
        "        text.append(t['text'])\n",
        "        # Add user\n",
        "        user.append(t['user']['name'])\n",
        "\n",
        "        \n",
        "\n",
        "    d = {'text': text,\n",
        "                \n",
        "         'user': user      \n",
        "         }\n",
        "\n",
        "    return pd.DataFrame(data=d)\n",
        "\n",
        "tweets_df =  tweets_to_df(\"Airlines.json\")\n",
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the text and classify the polarity of a tweet"
      ],
      "metadata": {
        "id": "4NxraWXlBl2u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5Q5TM4M25db"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    '''\n",
        "    Utility function to clean the text in a tweet by removing\n",
        "    links and special characters using regex.\n",
        "    '''\n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "\n",
        "\n",
        "def analize_sentiment(tweet):\n",
        "    '''\n",
        "    Utility function to classify the polarity of a tweet\n",
        "    using textblob.\n",
        "    '''\n",
        "    analysis = TextBlob(clean_tweet(tweet))\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 1\n",
        "    elif analysis.sentiment.polarity == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KgDf8Xu25dc"
      },
      "outputs": [],
      "source": [
        "tweets_df['SA'] = np.array([analize_sentiment(text) for text in tweets_df['text']])\n",
        "\n",
        "\n",
        "pos_tweets = [ text for index, text in enumerate(tweets_df['text']) if tweets_df['SA'][index] == 1]\n",
        "neu_tweets = [ text for index, text in enumerate(tweets_df['text']) if tweets_df['SA'][index] == 0]\n",
        "neg_tweets = [ text for index, text in enumerate(tweets_df['text']) if tweets_df['SA'][index] == 2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYlKWPnq25dc"
      },
      "outputs": [],
      "source": [
        "tweets_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocesing"
      ],
      "metadata": {
        "id": "X4gJ3nHOB0BG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-OKEJ3k25dc"
      },
      "outputs": [],
      "source": [
        "custom_stop_words = ['i','me', 'my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",'your','yours','yourself','yourselves','he','him','his','himself','she',\"she's\",'her','hers','herself','it',\"it's\",'its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that',\"that'll\",'these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','then','once','here','there','when','where','why','how','all','any','both','no','nor','not','only','own','same','so','s','t','can','will','just','don',\"don't\",'should',\"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\",\"to\"]\n",
        "stop_words = set(custom_stop_words)\n",
        "nltk.download('punkt')\n",
        "def tokenizer(s):\n",
        "    s = re.sub(r'http\\S+', '', s)\n",
        "    s = re.sub(r'\\$(\\w+)','',s)\n",
        "    translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
        "    s = s.translate(translate_table)\n",
        "    tokens = nltk.word_tokenize(s)\n",
        "    filtered_tokens =[]\n",
        "    for word in tokens:\n",
        "        if word.lower() not in stop_words:\n",
        "            filtered_tokens.append(word.lower())\n",
        "    return filtered_tokens\n",
        "\n",
        "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n",
        "\n",
        "tweets_df['TweetText'] = tweets_df['text'].apply(tokenizer)\n",
        "tweets_df['text_length'] = tweets_df['text'].apply(len)\n",
        "tweets_df = tweets_df[(tweets_df['text_length']>0)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ozQrjwm25dd"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "text = np.array(list(chain(*tweets_df['text'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHnH5s3P25de"
      },
      "outputs": [],
      "source": [
        "words= []\n",
        "for text in tweets_df.TweetText:\n",
        "    for word in text:\n",
        "        words.append(word)\n",
        "\n",
        "from collections import Counter\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "#len(vocab_to_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "ZA8H7eEAB7vX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O8DPsUI00m0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Use tf-idf features\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf = tfidf_vectorizer.fit_transform(text)\n",
        "\n",
        "# Use tf features\n",
        "tf_vectorizer = CountVectorizer()\n",
        "tf = tf_vectorizer.fit_transform(text)\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"Number of total features: {}\".format(len(tfidf_feature_names)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn03WFri00m1"
      },
      "outputs": [],
      "source": [
        "feature_names = np.array(tfidf_feature_names)\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iy8KWNZ00m1"
      },
      "outputs": [],
      "source": [
        "# Initialize Ida\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(max_iter=10,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                n_components=10,\n",
        "                                random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLyGgKlk00m1"
      },
      "outputs": [],
      "source": [
        "lda_tfidf = lda.fit(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvPHkZuX00m1"
      },
      "outputs": [],
      "source": [
        "lda_W = lda_tfidf.transform(tfidf)\n",
        "Counter([np.argmax(i) for i in lda_W])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLira2t_25dg"
      },
      "outputs": [],
      "source": [
        "max_len = 50\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    vec = []\n",
        "    zeroes = max_len - len(x)\n",
        "    if zeroes>0:\n",
        "        for i in range(zeroes):\n",
        "            vec.append(0)\n",
        "    \n",
        "    for i in range(max_len-len(vec)):\n",
        "        word = x[i]\n",
        "        vec.append(vocab_to_int[word])\n",
        "    return np.array(vec)\n",
        "\n",
        "tweets_df['encoded_text'] = tweets_df['TweetText'].apply(fun)\n",
        "\n",
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w43-8ajp00m2"
      },
      "outputs": [],
      "source": [
        "num_top_words = 15\n",
        "tp = np.zeros(shape=[0,5])\n",
        "\n",
        "def retrieve_top_words(model, feature_names, num_top_words):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic #{}:\".format(idx),topic, end='\\n')\n",
        "        tp = topic[:-num_top_words - 1:-1]\n",
        "        print(tp)\n",
        "    imp_feat = []\n",
        "    for vec in tweets_df.encoded_text:\n",
        "        imp_feat.append(vec.tolist())\n",
        "    imp_feat =np.array(imp_feat)\n",
        "    return imp_feat\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0S-fYt900m2"
      },
      "outputs": [],
      "source": [
        "features = retrieve_top_words(lda_tfidf, tfidf_feature_names, num_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxpdjjw-00m2"
      },
      "outputs": [],
      "source": [
        "p = len(pos_tweets)*100/len(tweets_df['TweetText'])\n",
        "n = len(neg_tweets)*100/len(tweets_df['TweetText'])\n",
        "nu = len(neu_tweets)*100/len(tweets_df['TweetText'])\n",
        "#print(tweets_df.head(10))\n",
        "print(\"Percentage of positive tweets: {}%\".format(p))\n",
        "print(\"Percentage of neutral tweets: {}%\".format(nu))\n",
        "print(\"Percentage de negative tweets: {}%\".format(n))\n",
        "\n",
        "colors = ['green', 'red', 'grey']\n",
        "sizes = [p,n,nu]\n",
        "labels = 'Positive', 'Negative', 'Neutral'\n",
        "\n",
        "## use matplotlib to plot the chart\n",
        "plt.pie(\n",
        "   x=sizes,\n",
        "   shadow=True,\n",
        "   colors=colors,\n",
        "   labels=labels,\n",
        "   startangle=90\n",
        ")\n",
        "\n",
        "plt.title(\"Sentiment Analysis.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSXI0RFt00m2"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "# !pip install PyPrind\n",
        "from keras.layers import Embedding,Dropout, LSTM,Bidirectional\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.layers import *\n",
        "# from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# import pyprind\n",
        "#max_features = 15000\n",
        "# cut texts after this number of words\n",
        "# (among top max_features most common words)\n",
        "maxlen = 500\n",
        "batch_size = 64\n",
        "max_num_words = 20000\n",
        "max_length = 500\n",
        "print('Loading data...')\n",
        "x = features\n",
        "y = tweets_df['SA'].values \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33) \n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNcYNIVB00m3"
      },
      "outputs": [],
      "source": [
        "# from EMARectified_adam import EMARectifiedAdam\n",
        "# # tf.gather(list, tf_look_up[index])\n",
        "# optm = EMARectifiedAdam(lr=1e-3)\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_num_words, 100, input_length=max_length))\n",
        "# model.add(SeqSelfAttention(attention_activation='relu'))\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=False, dropout=0.2)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "ePU8a7XbVSAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pred=np.argmax(y_pred,axis=1)"
      ],
      "metadata": {
        "id": "H_SRWajtXAcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atlE4rHw00m3"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmH-5jTU00m3"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "target_names = ['Neutral', 'Positive', 'Negative']\n",
        "print(\"ACCURACY is:\",metrics.accuracy_score(y_test, lstm_pred)) \n",
        "print(\"Confusion Matrix\",metrics.confusion_matrix(y_test, lstm_pred))\n",
        "print(\"Classification_Report is:\\n\",metrics.classification_report(y_test, lstm_pred, target_names=None))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, lstm_pred))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, lstm_pred))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, lstm_pred))) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "# actual = numpy.random.binomial(1,.9,size = 1000)\n",
        "# predicted = numpy.random.binomial(1,.9,size = 1000)\n",
        "\n",
        "# confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, lstm_pred)\n",
        "\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FkDjea_7-wt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BCQcRY725di"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(random_state=1, max_iter=30).fit(x_train, y_train)\n",
        "mlp_pred=clf.predict(x_test)\n",
        "print(\"ACCURACY is:\",metrics.accuracy_score(y_test, mlp_pred)) \n",
        "print(\"Confusion Matrix\",metrics.confusion_matrix(y_test, mlp_pred))\n",
        "print(\"Classification_Report is:\\n\",metrics.classification_report(y_test, mlp_pred, target_names=target_names))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, mlp_pred))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, mlp_pred))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, mlp_pred))) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, mlp_pred)\n",
        "\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy8MPxFS_hTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_clf = SVC(C=3,random_state=0).fit(x_train, y_train)\n",
        "svm_pred=svm_clf.predict(x_test)\n",
        "print(\"ACCURACY is:\",metrics.accuracy_score(y_test, svm_pred)) \n",
        "print(\"Confusion Matrix\",metrics.confusion_matrix(y_test, svm_pred))\n",
        "print(\"Classification_Report is:\\n\",metrics.classification_report(y_test, svm_pred, target_names=target_names))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, svm_pred))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, svm_pred))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, svm_pred))) "
      ],
      "metadata": {
        "id": "9e9LpTALFGs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "# actual = numpy.random.binomial(1,.9,size = 1000)\n",
        "# predicted = numpy.random.binomial(1,.9,size = 1000)\n",
        "\n",
        "# confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, svm_pred)\n",
        "\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SEyyFEOY-Ssv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}